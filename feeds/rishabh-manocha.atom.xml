<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Rishabh's blog - Rishabh Manocha</title><link href="/" rel="alternate"></link><link href="/feeds/rishabh-manocha.atom.xml" rel="self"></link><id>/</id><updated>2018-01-27T20:00:00-08:00</updated><entry><title>Searching the Oakland Library for books from your Goodreads Shelf</title><link href="/oakland-lib-goodreads-searcher.html" rel="alternate"></link><published>2018-01-27T20:00:00-08:00</published><updated>2018-01-27T20:00:00-08:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2018-01-27:/oakland-lib-goodreads-searcher.html</id><summary type="html">&lt;p&gt;A utility to search the Oakland library Catalog for available books from your Goodreads "to-read" shelf&lt;/p&gt;</summary><content type="html">&lt;p&gt;My only new years resolution this year was to read more books. Over the past couple of years, I had 
essentially stopped reading books, and that realization was not a joyous one. Reading books 
is a fun way to both learn and be entertained. Jhumpa Lahiri had a 
&lt;a href="https://www.goodreads.com/quotes/76236-that-s-the-thing-about-books-they-let-you-travel-without"&gt;good quote&lt;/a&gt; 
in The Namesake: &lt;code&gt;That's the thing about books. They let you travel without moving your feet.&lt;/code&gt; 
I'm happy to say, that so far, I've maintained my resolution, having finished 
&lt;a href="https://www.goodreads.com/book/show/4865.How_to_Win_Friends_and_Influence_People"&gt;two&lt;/a&gt; 
&lt;a href="https://www.goodreads.com/book/show/7069.The_World_According_to_Garp"&gt;books&lt;/a&gt; already 
and am onto my &lt;a href="https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow"&gt;third one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyways, since I have a subscription to the Oakland Library, I figured it'd be better for me to borrow 
the books I want to read as opposed to buying them each time (even though it's fun to fill out a 
bookshelf). At the same time, I started keeping track of books I want to read on 
&lt;a href="https://www.goodreads.com/"&gt;Goodreads&lt;/a&gt; (the shelves they have, along with the ability to order 
the books makes it a useful utility for me). The first book I borrowed from the library was 
Thinking Fast and Slow and the process of me finding a copy to borrow left me wanting for a 
better way to locate and hold a book. The story goes something like this: find the 
next book I want to read from my GR shelf, search on the oakland library website for it's 
availability, find out that it's not available, rinse and repeat until one
becomes available (took me a few days). The putting it on hold was particulary important 
since I had one occasion where I saw the book was available at a particular branch, 
immdiately left to go pick it up (without putting it on hold) and by the time I 
reached there (~15 minutes), someone had already borrowed it. I eventually did find a copy 
at a different branch but I really wanted a better way of doing things.&lt;/p&gt;
&lt;p&gt;The solution to my problem was to build an API that accesses the Oakland Library Catalog (they 
don't really have an API, so this library scrapes their website) and combine it with the 
published API from GR (xml, really) to tell me whether the books on my shelf are available 
at the library or not (and more in the future). At the same time, working on this project 
gave me the opportunity to learn and use Python3, something I hadn't really done until now 
(for no good reason besides laziness).&lt;/p&gt;
&lt;h1&gt;Building an "API" for the Oakland Library Catalog&lt;/h1&gt;
&lt;p&gt;To be able to search their catalog programatically, I had to write a simple scraper. This 
was done using the awesome &lt;a href="http://docs.python-requests.org/en/master/"&gt;requests&lt;/a&gt; library 
for making the http requests and the equally awesome 
&lt;a href="https://www.crummy.com/software/BeautifulSoup/"&gt;BeautilfulSoup&lt;/a&gt; library to parse the HTML 
response to find the elements I'm looking for. The code for all of this is available on 
&lt;a href="https://github.com/rmanocha/oaklibsearcher/blob/master/oaklibapi/api.py"&gt;github&lt;/a&gt; and 
should be fairly easy to follow. The &lt;em&gt;oaklibapi&lt;/em&gt; module is intended to be an API module 
though and, if needed, can be imported into other projects as well. Some sample code showing 
how to use it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;oaklibapi&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OaklandLibraryAPI&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;oaklib&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OaklandLibraryAPI&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;isbn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;9780060892999&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# you can only search by the book&amp;#39;s ISBN&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;oaklib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# check if the book is available&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;oaklib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# retrieve the title as returned by the Oakland Library Catalog&lt;/span&gt;
&lt;span class="n"&gt;Out&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;A canticle for Leibowitz&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's all there is to it as of now. I need to add a couple more enhancements, specifically 
around &lt;a href="https://github.com/rmanocha/oaklibsearcher/issues/3"&gt;retreiving the number of books available&lt;/a&gt; 
as well as &lt;a href="https://github.com/rmanocha/oaklibsearcher/issues/6"&gt;the branches they're available at&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An interesting bit to note here is that BeautifulSoup since v4 has started giving users the option to 
specify a parser. I ended up using the the &lt;a href="http://lxml.de/"&gt;lxml parser&lt;/a&gt; for this since I had to use it 
to parse XML from the GR api anyways. I didn't benchmark things, but as documented, and based on my 
very non-scientific observations, I did perceive an improvement in instantiating the &lt;code&gt;OaklandLibraryAPI&lt;/code&gt; 
class.&lt;/p&gt;
&lt;h1&gt;Integrating with the Goodreads API&lt;/h1&gt;
&lt;p&gt;Fortunately, GR does &lt;a href="https://www.goodreads.com/api"&gt;publish an API&lt;/a&gt;. It does seem rather antiquated 
though - responses are in XML (some are in JSON, which is weird too; why do only some of the 
resources return either XML or JSON while most only return XML) and there are no official 
client libraries. There are &lt;a href="https://github.com/sefakilic/goodreads/"&gt;libraries&lt;/a&gt; 
&lt;a href="https://github.com/dasevilla/goodreads-python"&gt;available&lt;/a&gt; on github. None of them really did what 
I wanted them to do though (albeit, I only spent a short amount of time looking) and since it was 
fairly easy to query the API and parse the response myself, I just ended up doing so, instead of 
using one of the available libraries.&lt;/p&gt;
&lt;p&gt;My goal with this API was to simply retrieve my 
&lt;a href="https://www.goodreads.com/review/list/75811584?shelf=to-read"&gt;"to-read" bookshelf&lt;/a&gt;. It took me 
some time to figure out that I had to use the &lt;a href="https://www.goodreads.com/api/index#reviews.list"&gt;reviews.list&lt;/a&gt; 
resource for this (go figure). Only other thing to note here that BeautifulSoup does not seem to have a 
default parser for XML documents. I had to explicitly define &lt;em&gt;lxml&lt;/em&gt; while parsing the response from the API 
(as opposed to HTML, which does seem to have a default parser, which does throw a big &amp;amp; ugly warning when 
not overridden though).&lt;/p&gt;
&lt;p&gt;Here's some sample code on how to use &lt;em&gt;GoodreadsQueryAPI&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;goodreads_api&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GoodreadsQueryAPI&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;gr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GoodreadsQueryAPI&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;75811584&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;access_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# insert your GR API access key for the second param&lt;/span&gt;

&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;book&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;gr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_books&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
   &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;     &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780802123459&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;The Sympathizer&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780525427575&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Enlightenment Now: The Case for Reason, Science, Humanism, and Progress&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780670022953&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;The Better Angels of Our Nature: Why Violence Has Declined&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780812536355&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;A Deepness in the Sky (Zones of Thought, #2)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;The Wandering Earth: Classic Science Fiction Collection&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780765309402&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Old Man&amp;#39;s War (Old Man&amp;#39;s War, #1)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9781585422784&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Ultramarathon Man: Confessions of an All-Night Runner&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780316547611&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;The Power&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;We Are Legion (We Are Bob) (Bobiverse, #1)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Split Second&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9781101886724&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Waking Gods (Themis Files, #2)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9781594631764&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;And the Mountains Echoed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780062273208&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;The Hard Thing About Hard Things: Building a Business When There Are No Easy Answers&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9781501126062&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sing, Unburied, Sing&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780670026197&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;A Gentleman in Moscow&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9780553447439&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Evicted: Poverty and Profit in the American City&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9781594204876&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Grant&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;isbn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;9781501144318&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Why We Sleep: Unlocking the Power of Sleep and Dreams&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Of-course, many improvements can be made to the interface, but this works for a v0.1 :). I think it'd also be good to 
be able to pull &lt;a href="https://github.com/rmanocha/oaklibsearcher/issues/1"&gt;the name of the author&lt;/a&gt; as well as ordering 
the books in the response by &lt;a href="https://github.com/rmanocha/oaklibsearcher/issues/10"&gt;position defined by the user&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Putting it all together&lt;/h1&gt;
&lt;p&gt;Having the API for the library and Goodreads makes it easy for us to put everything together. Grabbing the list of 
books in the GR shelf followed by querying each one to see it's availability is 
&lt;a href="https://github.com/rmanocha/oaklibsearcher/blob/master/oaklibsearcher.py"&gt;done already&lt;/a&gt;. 
&lt;em&gt;GOODREADS_ACCESS_KEY&lt;/em&gt; and &lt;em&gt;GOODREADS_USER_ID&lt;/em&gt; is read in from &lt;em&gt;settings.py&lt;/em&gt; (a template for that is 
&lt;a href="https://github.com/rmanocha/oaklibsearcher/blob/master/settings.py-def"&gt;available&lt;/a&gt;). For now, it simply 
spits out the list of books along with it's availability (like below). A good v1.0 would be to implement 
some form of a &lt;a href="https://github.com/rmanocha/oaklibsearcher/issues/11"&gt;notification system&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;oaklibapi&lt;span class="o"&gt;)&lt;/span&gt; ~ &lt;span class="o"&gt;[&lt;/span&gt;master &lt;span class="o"&gt;]&lt;/span&gt; $ python oaklibsearcher.py
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The Sympathizer, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780802123459&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The sympathizer : a novel is available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Enlightenment Now: The Case &lt;span class="k"&gt;for&lt;/span&gt; Reason, Science, Humanism, and Progress, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780525427575&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;ENLIGHTENMENT NOW : THE CASE FOR REASON, SCIENCE, HUMANISM, AND PROGRESS is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The Better Angels of Our Nature: Why Violence Has Declined, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780670022953&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The better angels of our nature : why violence has declined is available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;A Deepness in the Sky &lt;span class="o"&gt;(&lt;/span&gt;Zones of Thought, &lt;span class="c1"&gt;#2), ISBN=9780812536355&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The Wandering Earth: Classic Science Fiction Collection, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
No ISBN available. Skipping
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Old Man&lt;span class="s1"&gt;&amp;#39;s War (Old Man&amp;#39;&lt;/span&gt;s War, &lt;span class="c1"&gt;#1), ISBN=9780765309402&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Old man&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s war is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Ultramarathon Man: Confessions of an All-Night Runner, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9781585422784&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Ultramarathon man : confessions of an all-night runner is available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The Power, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780316547611&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The power : a novel is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;We Are Legion &lt;span class="o"&gt;(&lt;/span&gt;We Are Bob&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;Bobiverse, &lt;span class="c1"&gt;#1), ISBN=&lt;/span&gt;
No ISBN available. Skipping
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Split Second, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
No ISBN available. Skipping
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Waking Gods &lt;span class="o"&gt;(&lt;/span&gt;Themis Files, &lt;span class="c1"&gt;#2), ISBN=9781101886724&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Waking gods is available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;And the Mountains Echoed, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9781594631764&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;And the mountains echoed is available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The Hard Thing About Hard Things: Building a Business When There Are No Easy Answers, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780062273208&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;The hard thing about hard things : building a business when there are no easy answers is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Sing, Unburied, Sing, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9781501126062&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Sing, unburied, sing : a novel is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;A Gentleman in Moscow, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780670026197&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;A gentleman in Moscow is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Evicted: Poverty and Profit in the American City, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9780553447439&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Evicted : poverty and profit in the American city is available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Grant, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9781594204876&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Grant is not available
Looking &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Why We Sleep: Unlocking the Power of Sleep and Dreams, &lt;span class="nv"&gt;ISBN&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;9781501144318&lt;/span&gt;
Book with &lt;span class="nv"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Why we sleep : unlocking the power of sleep and dreams is not available
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That's about all there is to it, for now. There are quiet a few improvements that can be made to it and I'll be working on 
a few of them over the coming weeks; I'll post a follow up as and when I learn some new things.&lt;/p&gt;</content><category term="hobby-projects"></category><category term="python3"></category></entry><entry><title>Using contracts for Data Warehouse batch ETL processes</title><link href="/contracts-data-warehouse-batch-etl.html" rel="alternate"></link><published>2017-12-22T20:00:00-08:00</published><updated>2017-12-22T20:00:00-08:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2017-12-22:/contracts-data-warehouse-batch-etl.html</id><summary type="html">&lt;p&gt;using-contracts-data-warehouse-batch-etl&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Like many other companies, we have a Data Warehouse that brings together
all our production data sources into a single location to enable various 
business functions such as business intelligence, data analytics, 
experiment analysis etc.&lt;/p&gt;
&lt;p&gt;For a long time, we have done this by copying our production postgres 
tables using a batch ETL process (we have in house scripts running 
on a scheduler to do the copies every 'n' hours) without any transformations. 
Consumers of the data warehouse, in turn, have used these tables to perform all of the 
operations listed above. &lt;/p&gt;
&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;While this process has served us well, as we have moved to 
&lt;a href="https://www.upwork.com/blog/2017/01/upwork-modernization/"&gt;modernize our technology stack&lt;/a&gt;, 
this approach has started to show some limitations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One of the guiding principles of good software design is 
&lt;a href="https://martinfowler.com/articles/microservice-trade-offs.html#boundaries"&gt;strong module boundaries&lt;/a&gt; 
which allows one business domain to be designed and evolve independently of other domains. The systems 
within these domains interact with each other over contracts at the http layer, not the data layer.
By copying data maintained by these systems verbatim to the data warehouse (which represents a boundary), 
we end up crossing these boundaries leading to the inadvertant coupling of these production and 
data warehouse systems. This issue manifests itself in many areas:&lt;ul&gt;
&lt;li&gt;Any migrations on the table or changes in the logic of the fields stored in the tables are 
   blocked by the fact that these changes need to be propagated to the data warehouse and all 
   consumers of these tables need to migrate their processes to account for them before
   they can be released to our end users.&lt;/li&gt;
&lt;li&gt;Non product concerns (whether an entity was created from the mobile or desktop site, for example) 
   leak into the production data models since these tables are, in turn, used for BI and related
   processes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;As an engineering team, we're starting to adopt high scalability/availability data stores (DynamoDB) or 
   versatile data stores (MongoDB, Cloudsearch). Since our ETL jobs can only use SQL interfaces (postgres), 
   the data from these stores needs to be moved to a compatible data store, which usually means that the schema 
   exposed won't be a verbatim copy of the production data models in these cases.&lt;/li&gt;
&lt;li&gt;Because we copy the production data models directly into the data warehouse, we lack a consistent schema that 
   consumers of the data warehouse can use, suggest improvements for, ask questions of etc. The choices of each 
   product team, instead, are exposed to these consumers leading to difficulities in understanding the data exposed
   via each table (inconsistent column names and types, difficulties in understanding how to join tables, etc.). 
   Furthermore, as we move more of our domains to be built as finer grained services, we naturally end up denormalizing
   the data, spreading it across multiple tables maintained by different services. This leads to confusion in the 
   data warehouse since it's hard to know which table should be used as the trusted source for any given 
   piece of data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, this process has worked well for us, especially when we had a monolithic system 
with a few handful of databases and tables. However, as we continue to scale in usage of the data warehouse, 
in our technology as well as organizationally, our understanding of the drawbacks of the existing system as 
well as the best practices involved in operating a data warehouse have evolved.&lt;/p&gt;
&lt;p&gt;An example of the drawbacks manifesting themselves came about earlier this year when we were wrapping up a rather 
big modernization project. Modernization, for us, is not simply a rewrite from our legacy (Perl) stack to our 
new (JVM based) stack but a complete rethinking of the bounded context (BC) we're working on. As such, this project 
involved, among other things, a complete rethinking of the data model of BC in question (I'll 
write more about this process, the choices we made, the difficulties we faced as well as the mistakes we made 
in a later post). Since this was an existing BC, we already had a lot of internal processes spread across disparate 
teams (marketing, analytics, product, customer support, data science and more) using the data from the existing 
data model. And since we used to expose the raw tables from our legacy data model to these users, all their work was 
tied directly to this one table. Because of this, when the time came to switch to the new data model, we not only had 
to work with multiple teams to update their scripts but also had to ensure that the new processes they write are not 
directly tied to the new tables in our production system we had built.&lt;/p&gt;
&lt;h1&gt;Solution - Purgatory&lt;/h1&gt;
&lt;p&gt;To solve the issues described above, we introduced and enforced boundaries between the data flows in our ETL jobs 
as well as hand offs at various steps in these flows. The new workflow is described in the diagram below.&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/purgatory_data_flow_responsibilities_handoffs.png"&gt;&lt;img src="/images/purgatory_data_flow_responsibilities_handoffs.png" style="max-width: 100%" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The diagram above describes a product data model for the given BC (spread across multiple databases and multiple tables within each
DB). This process requires that each development team of any given BC (and services within) will be responsible for creating, 
running and maintaining an ETL job that feeds a view of their data in purgatory. The only consumer of this data will be the 
data warehouse ETL job (and the team that owns it) which will, in turn, be responsible for extracting this data and exposing 
it to their consumers in a schema that is consistent over time and where the data is reconciled at regular intervals.&lt;/p&gt;
&lt;p&gt;The introduction of purgatory allows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Product models to evolve independently from the data warehouse schema. The contract the product team exposes in purgatory 
   requires them to ensure that any changes are backwards compatible (just as they do with their http contracts). In addition, 
   any changes to the contract exposed via purgatory only needs to be coordinated with the data warehouse team and their ETL job 
   as opposed to the entire data warehouse community of users.&lt;/li&gt;
&lt;li&gt;Separation of concerns for the product teams and the services they are building. These efforts can be focused on building 
   (testing, releasing, A/B testing etc.) the product as well as maintaining the contracts with other product consumers 
   (including purgatory) without having to focus on the impact of the changes to the data warehouse consumers (marketing, 
   analytics, product, finance, trust &amp;amp; safety etc. - all of whom have different requirements).&lt;/li&gt;
&lt;li&gt;Consistent ETL pipelines that various teams can reuse to expose data in purgatory.&lt;/li&gt;
&lt;li&gt;A single location to vet and reconcile our data before exposting it to users of the data warehouse.&lt;/li&gt;
&lt;li&gt;Allows us to define a consistent schema for UDW consumers&lt;ul&gt;
&lt;li&gt;In the future, this process will also allow us to define multiple schemas and dimensions specific to the 
   questions being asked within the data warehouse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Implementation and Results&lt;/h1&gt;
&lt;p&gt;We ended up creating the purgatory ETL jobs using &lt;a href="https://aws.amazon.com/datapipeline/"&gt;AWS Data Pipeline&lt;/a&gt; (a subject 
for another blog post) and delivering the data to the data warehouse ETL jobs in an RDS Postgres database. The second 
ETL job only cares about the one table we expose to the data warehouse team where the data is combined from the 
disparate databases and tables the production services use.&lt;/p&gt;
&lt;p&gt;This process has been running for the last 6 months at this point, and has been adopted by many other teams and projects 
internally to do the same. The outcome of this has, so far, been encouraging. One of the vertical teams, for example, was 
building an MVP of a product to see it's market viability. For such projects, time to market is a big concern, and so, we 
ended up using postgres as a key value store (values being a serialized thrift entity stored in a jsonb column) to launch 
as soon as we could. While this worked very well for the product, it would not have worked well for the data warehouse since 
querying a jsonb column isn't exactly easy. Instead of making a simple copy of the table(s) in question, we copied our existing 
AWS data pipeline jobs, made modifications to the transformation scripts, agreed with the data warehouse team on a set of contracts 
(tables) we'd deliver to them and let them work on transforming the data from these tables into the final versions exposed in 
the data warehouse to be used by the analytics and product teams to understand the efficacy of the new solution we'd launched.&lt;/p&gt;</content><category term="systems-design"></category></entry><entry><title>Getting started with EMR Spark Steps</title><link href="/aws-emr-spark-steps.html" rel="alternate"></link><published>2017-11-25T14:00:00-08:00</published><updated>2017-11-25T14:00:00-08:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2017-11-25:/aws-emr-spark-steps.html</id><summary type="html">&lt;p&gt;How to think about and use Spark Steps in EMR&lt;/p&gt;</summary><content type="html">&lt;p&gt;Let's talk a little bit about &lt;a href="http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html"&gt;EMR Spark Steps&lt;/a&gt;. 
This is the recommended way to kick off spark jobs in EMR. Well, recommended at-least for streaming jobs (since that's all 
I have experience with so far).&lt;/p&gt;
&lt;p&gt;When running a spark job in standalone mode, you usually end up doing so by using &lt;em&gt;spark-submit&lt;/em&gt; after downloading your 
spark distribution. &lt;/p&gt;
&lt;p&gt;That command usually ends up looking something like this (the parameters passed in are not important for now):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/spark-submit --packages org.apache.spark:spark-streaming-kinesis-asl_2.11:2.2.0 ~/spark/job.py stream
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The EMR step does something very similar. Only differnece (for me, anyways) is that I don't need to ssh into the master node 
and kick the job off manually. In addition, by doing so, you get to see your steps and their status in the EMR cluster's dashboard 
on the AWS UI.&lt;/p&gt;
&lt;p&gt;Now, here's what starting the cluster via the AWS UI would look like: &lt;/p&gt;
&lt;p&gt;&lt;a href="/images/emr_spark_step_populated.png"&gt;&lt;img src="/images/emr_spark_step_populated.png" style="max-width: 100%" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The steps I followed to get this to work as I expected it to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the Create Cluster screen, click on "Go to advanced options" at the top (there's a link next to the &lt;em&gt;Quick Options&lt;/em&gt; text)&lt;/li&gt;
&lt;li&gt;Once you are on the &lt;em&gt;Advanced Options&lt;/em&gt; screen, you can select the EMR version as well as the applications you want your cluster to have. 
   One of these has to be Spark, since we're starting a spark streaming app.&lt;/li&gt;
&lt;li&gt;Now, under the &lt;em&gt;Add Steps&lt;/em&gt; section, you want to select the &lt;em&gt;Spark Application&lt;/em&gt; step type and click configure.&lt;/li&gt;
&lt;li&gt;In the dialog that opens next, we start filling in the various options shown in the screenshot above&lt;ul&gt;
&lt;li&gt;You can give your step a name. I haven't ever changed the default so far, since I'm only running a single job on the cluster for now. &lt;/li&gt;
&lt;li&gt;I haven't ever changed the deploy mode either. The help instructions there give you some info, so feel free to give it a shot :)&lt;/li&gt;
&lt;li&gt;Now comes the interesting bits. Spark-submit options are the options you'd want to pass in to the spark-submit command.
  This is where you'd pass in, for example, the kinesis streaming jar (since it doesn't come pre-packaged with spark) as I've 
  done in the screenshot above.&lt;/li&gt;
&lt;li&gt;Application location is where you've put your jar or python script that you'd want spark to execute. As is usual with many AWS 
  services, they want this to be in S3.&lt;/li&gt;
&lt;li&gt;Arguments are the parameters you want to pass in to your streaming application. Usually, this would include the stream name you want to 
  target or the environment this job is running in etc (I haven't figured out how to set the environment as a system variable on each node in 
  the cluster).&lt;/li&gt;
&lt;li&gt;Finally, the &lt;em&gt;Action on Failure&lt;/em&gt; option let's you specify what you want to have happen with the cluster when the step fails. The options
  I have tried out are &lt;em&gt;Terminate&lt;/em&gt; and &lt;em&gt;Continue&lt;/em&gt; both of which do what you'd expect them to do (i.e. the former shuts down the EMR cluster 
  and the latter keeps it running).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can of-course, do all this via the CLI as well. The command to do this looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;aws emr create-cluster --termination-protected --applications &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Hadoop &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Zeppelin &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Spark &lt;span class="nv"&gt;Name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;Ganglia --release-label emr-5.9.0 --steps &lt;span class="s1"&gt;&amp;#39;[{&amp;quot;Args&amp;quot;:[&amp;quot;spark-submit&amp;quot;,&amp;quot;--deploy-mode&amp;quot;,&amp;quot;cluster&amp;quot;,&amp;quot;--packages&amp;quot;,&amp;quot;org.apache.spark:spark-streaming-kinesis-asl_2.11:2.2.0&amp;quot;,&amp;quot;s3://bucket/job.py&amp;quot;,&amp;quot;params1&amp;quot;, &amp;quot;params2&amp;quot;],&amp;quot;Type&amp;quot;:&amp;quot;CUSTOM_JAR&amp;quot;,&amp;quot;ActionOnFailure&amp;quot;:&amp;quot;CONTINUE&amp;quot;,&amp;quot;Jar&amp;quot;:&amp;quot;command-runner.jar&amp;quot;,&amp;quot;Properties&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;Name&amp;quot;:&amp;quot;Spark application&amp;quot;}]&amp;#39;&lt;/span&gt; --instance-groups &lt;span class="s1"&gt;&amp;#39;[{&amp;quot;InstanceCount&amp;quot;:1,&amp;quot;InstanceGroupType&amp;quot;:&amp;quot;MASTER&amp;quot;,&amp;quot;InstanceType&amp;quot;:&amp;quot;m3.xlarge&amp;quot;,&amp;quot;Name&amp;quot;:&amp;quot;Master - 1&amp;quot;},{&amp;quot;InstanceCount&amp;quot;:2,&amp;quot;InstanceGroupType&amp;quot;:&amp;quot;CORE&amp;quot;,&amp;quot;InstanceType&amp;quot;:&amp;quot;m3.xlarge&amp;quot;,&amp;quot;Name&amp;quot;:&amp;quot;Core - 2&amp;quot;}]&amp;#39;&lt;/span&gt; --auto-scaling-role EMR_AutoScaling_DefaultRole --ebs-root-volume-size &lt;span class="m"&gt;10&lt;/span&gt; --service-role EMR_DefaultRole --enable-debugging --name &lt;span class="s1"&gt;&amp;#39;RMSparkCluster&amp;#39;&lt;/span&gt; --scale-down-behavior TERMINATE_AT_INSTANCE_HOUR --region us-west-2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The only interesting part about this is how to pass in the options to the &lt;em&gt;--steps&lt;/em&gt; parameter. You do this with individual strings in the list you pass in; not sure why this is.&lt;/p&gt;
&lt;p&gt;That's it - with this, you now have specific an EMR Spark Step that'll let you run your streaming job when your EMR cluster comes up. I've found 
that use Spark Steps makes it much easier to kick off Spark streaming jobs against a kinesis stream along and have things like metrics, status and the
number of jobs running.&lt;/p&gt;</content><category term="aws"></category><category term="emr"></category><category term="apache-spark"></category></entry><entry><title>Getting started with EMR + Apache Spark</title><link href="/aws-emr-spark-getting-started.html" rel="alternate"></link><published>2017-11-10T09:00:00-08:00</published><updated>2017-11-10T09:00:00-08:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2017-11-10:/aws-emr-spark-getting-started.html</id><summary type="html">&lt;p&gt;Some basic things to know about getting started with Apache Spark on AWS EMR&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's been a while since my last post. Given that I've been playing 
around with &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt; on 
&lt;a href="https://aws.amazon.com/emr/"&gt;AWS EMR&lt;/a&gt; lately, I figured I'd write 
about it.&lt;/p&gt;
&lt;p&gt;At work, we're trying to solve a specific problem: we want to know 
how many times has a particular user has taken a particular action over 
a rolling window of time (ranging from seconds to hours). Further, 
we want this data to be calculated in near real time so that actions
can be taken based on this "aggregation" by another process while the
user is navigating our site, as opposed to a few hours later (which 
is what we have done so far, since we rely on our data warehouse to 
be the source of information for the calculations). For 
example, at LinkedIn, we'd want to know how many times has a given 
user viewed another user's profile over the last 10 minutes. A 
simple use case for this aggregation might be to calculate the 99'th 
percentile of profile views over the 10 minute rolling window.&lt;/p&gt;
&lt;p&gt;I'll write a separate, and more detailed blog post about the architecural 
choices we made and what the tradeoffs were, but for now, 
I just wanted to list some things that I found interesting while working with 
EMR and Spark 2.2.0 (again, more details in separate (several ?) blog posts):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There's no easy way to load the Spark UI when running an EMR cluster. You 
have to setup an SSH tunnel to get in, and in our case, this caused issues since 
our VPC for the project (we have isolated VPCs so that we do not adversely impact 
our production workloads with our 'analytics' workloads, at-least not yet) 
won't allow us to ssh into machines in it from inside our corporate VPN (
I don't understand why, and I don't understand VPCs in general, really).&lt;/li&gt;
&lt;li&gt;The best way to actually kick off a Spark Streaming job is to use an 
&lt;a href="http://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-submit-step.html"&gt;EMR Spark Step&lt;/a&gt;. 
These allow you to define specific actions you want EMR to take once the cluster has been bootstrapped. 
You can also tell EMR to shutdown the cluster once all steps assigned to it have been executed.&lt;/li&gt;
&lt;li&gt;A good way to install packages your streaming job needs is to use 
&lt;a href="http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html"&gt;Bootstrap actions&lt;/a&gt;. For me,
I needed &lt;em&gt;boto3&lt;/em&gt; to be installed on the workers so that once my python each streaming batch was done, 
I could send the results into a kinesis stream.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll talk in more details about each one of these (and some other things) in follow up blog posts.&lt;/p&gt;</content><category term="aws"></category><category term="emr"></category><category term="apache-spark"></category></entry><entry><title>Experiments with RaspberryPi and iOS for home automation</title><link href="/rpi-ios-home-automation.html" rel="alternate"></link><published>2015-11-22T08:00:00-08:00</published><updated>2015-11-22T08:00:00-08:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2015-11-22:/rpi-ios-home-automation.html</id><summary type="html">&lt;p&gt;Some notes and thoughts on how to go about doing home automation using a RaspberryPi followed up by an iOS app to use the features.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever since I bought a couple of RaspberryPi's a year or so ago, I've been interested in doing some home automation with them. However, 
I never really got down to it (for various reasons; lazy, no easy tutorial available, parts missing, life etc.) until the past couple 
of months or so.&lt;/p&gt;
&lt;p&gt;I finally got around to buying a &lt;a href="http://www.amazon.com/gp/product/B00C8O9KHA"&gt;relay&lt;/a&gt; and 
&lt;a href="http://www.amazon.com/gp/product/B00M5WLZDW"&gt;female to female cables&lt;/a&gt;. With these in hand, I followed a few tutorials on YouTube
to get a light setup to be controlled by the GPIO pins and the relay (&lt;a href="https://www.youtube.com/watch?v=WpM1aq4B8-A"&gt;some&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=oaf_zQcrg7g"&gt;good&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=3u45htuQeag"&gt;videos&lt;/a&gt;).
In the end, the whole setup ends up looking something like the below (I'll go into more detail about this setup in a later post perhaps):&lt;/p&gt;
&lt;p&gt;&lt;img alt="RPI Setup" src="http://i.imgur.com/pt6aft9.jpg"&gt;&lt;/p&gt;
&lt;p&gt;With the wires all setup, the next step is to actual use the setup to be able to remotely turn on or off a light. This is where the &lt;a href="https://github.com/rmanocha/rpi_lights/blob/master/flask_light.py"&gt;flask
app&lt;/a&gt; I wrote comes in. As you might see, the GPIO pins being used are #9 and #2. 
These two represent the two ends of the relay board (in my setup anyways). Running this as a webserver on the raspberry pi (you need to run as root
otherwise the rpi is unable to access the GPIO pins) allows me to then make a simple GET request to &lt;code&gt;/toggle&lt;/code&gt; for one light and &lt;code&gt;/toggle_big&lt;/code&gt; for the other.&lt;/p&gt;
&lt;p&gt;Now, while I had an Android device, hitting these endpoints was easy. I simply setup a Tasker widget on my home screen that when touched would make a GET request
to one of these two endpoints which would, in turn, simply toggle the light on or off. However, now that I use an iOS device, Tasker (or any other widgets) are not an option
anymore (which sucks, IMO). This was a good opportunity to get my feet wet in the iOS development world. There are a bazillion tutorials online on how to get started, so I won't
go into them here. I would say that making a simple GET request via swift turned out to be a harder task than I had envisioned it to be (though I guess, python, natively makes it just
as hard). At the end of the day, &lt;a href="https://github.com/rmanocha/RPiLightApp/blob/master/RPiLight/ViewController.swift"&gt;this is what I came up with&lt;/a&gt;. Essentially a viewport with two buttons; each
one simply toggles the specific light on or off. As you can see from the image below, I have not figured out how to center the buttons in the view; not such a big deal for me for now.&lt;/p&gt;
&lt;p&gt;&lt;img alt="iOS App" src="http://i.imgur.com/8FQimr8.png"&gt;&lt;/p&gt;
&lt;p&gt;The next step for me is to get force touch working on my iPhone 6s so that the app does not need to open up for me to be able to turn the light on or off (similar to a widget on Android devices).
Additionally, I need to buy another relay board and a wireless dongle for the pi so that I can do the same thing for the lamps in my bedroom.&lt;/p&gt;
&lt;p&gt;So that's mostly it. This setup works well for now and gave me a good opportunity to play around with a few new and interesting technologies/languages.&lt;/p&gt;</content><category term="raspberrypi"></category><category term="ios"></category><category term="python"></category></entry><entry><title>Opsworks stack automation using Jenkins and AWS Boto</title><link href="/opsworks-boto-jenkins-automation.html" rel="alternate"></link><published>2014-07-30T08:00:00-07:00</published><updated>2014-07-30T08:00:00-07:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2014-07-30:/opsworks-boto-jenkins-automation.html</id><summary type="html">&lt;p&gt;A python script that allows us to automate the creation of new instances within an Opsworks stack via a Jenkins job.&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work, we've been using &lt;a href="http://docs.aws.amazon.com/cli/latest/reference/opsworks/index.html"&gt;AWS OpsWorks&lt;/a&gt; lately for a lof of our new infrastructure. It is a pretty neat tool to setup a deployment pipeline for your applications and allows for quiet a lot of customization (through the use of chef-solo recipes). How to use OpsWorks is out of the scope of this blog post. What I did want to talk about however, was how to use &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; and &lt;a href="http://aws.amazon.com/sdkforpython/"&gt;Boto&lt;/a&gt; to automate instance creation/deployment. The use case here is that not everyone has direct access to the AWS console or has keys setup to issue commands via the CLI. We still need to allow these devs to be able to deploy their code (be it a specific branch or simply the master branch) to an instance resembling production and ensure that things work correctly (and down the road, run automation tests).&lt;/p&gt;
&lt;p&gt;Luckliy, with the help of Boto and the Python script plugin for Jenkins, this is relatively easy. You obviously need to have a Jenkins server running along with the Python script plugin installed. Additionally, you need to install the boto library (I installed it globally) - usually via pip (&lt;code&gt;sudo pip install boto&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Once these requirements are met, make the required modifications to the script below, add a build step to execute a python script and voila.&lt;/p&gt;
&lt;p&gt;The process this script follows is documented inline, but the gist is that we do deployments by creating new instances and deleting old ones instead of running the &lt;code&gt;deploy&lt;/code&gt; command on an existing instance. This does take longer but I feel it's cleaner than deploying multiple times to the same instance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;boto&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="c1"&gt;# Fill in the various required fields below (should be self explanatory)&lt;/span&gt;
&lt;span class="n"&gt;AWS_ACCESS_KEY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;FILL_ME&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;AWS_SECRET_KEY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;FILL_ME&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;STACK_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;FILL_ME&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;APP_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;FILL_ME&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;LAYER_IDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;FILL_ME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;INSTANCE_TYPE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;m1.medium&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# We allow users to build &amp;#39;parametrized&amp;#39; builds - they can specify the branch they want to use&lt;/span&gt;
&lt;span class="c1"&gt;# to build the new instance. The APP is updated to use that branch and once the new instance is created,&lt;/span&gt;
&lt;span class="c1"&gt;# it&amp;#39;s updated again to rever to master (this piece is hardcoded)&lt;/span&gt;

&lt;span class="c1"&gt;# This is the format OpsWorks expects (they should clean it up and make the dict be more pythonic)&lt;/span&gt;
&lt;span class="n"&gt;APP_SOURCE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Url&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;FILL_ME&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;git&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Revision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;BRANCH&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;span class="n"&gt;APP_SOURCE_MASTER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;APP_SOURCE&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;APP_SOURCE_MASTER&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Revision&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;master&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;# default branch should always be master&lt;/span&gt;

&lt;span class="n"&gt;opsworks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;boto&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect_opsworks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aws_access_key_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;AWS_ACCESS_KEY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;aws_secret_access_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;AWS_SECRET_KEY&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Print out the instances running presently&lt;/span&gt;
&lt;span class="n"&gt;online_instances&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Hostname: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;, DNS: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;, Status: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Hostname&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;PrivateDns&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;PrivateDns&amp;#39;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe_instances&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stack_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;STACK_ID&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Instances&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Online Instances: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;online_instances&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Deploying branch: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;APP_SOURCE&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Revision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Update the app to use our specific branch&lt;/span&gt;
&lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update_app&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;app_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;APP_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;app_source&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;APP_SOURCE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Adding another instance&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# Creat the instance (we&amp;#39;ll later have to start it too)&lt;/span&gt;
&lt;span class="n"&gt;new_inst&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create_instance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stack_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;STACK_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer_ids&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LAYER_IDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;instance_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;INSTANCE_TYPE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;New Instance ID: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;. Starting it now&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;new_inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;InstanceId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# As promised, start the instance.&lt;/span&gt;
&lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start_instance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;InstanceId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Start command sent. Checking Status now&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# We need to start &amp;#39;describing&amp;#39; the instance to check it&amp;#39;s status. Depending on the type of instance created&lt;/span&gt;
&lt;span class="c1"&gt;# it can take a while to come up (m1.medium usually take 15-20 mins for me - anecdotal evidence though).&lt;/span&gt;
&lt;span class="n"&gt;inst&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe_instances&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;instance_ids&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;InstanceId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Instances&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;New Instance Hostname: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;, Status: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Hostname&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Wait until the instance is either up or the setup failed.&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;online&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;setup_failed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flush&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;inst&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe_instances&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;instance_ids&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;InstanceId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Instances&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Status is: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;. Sleeping for 30 seconds&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Use this to specify (to Jenkins) whether the job failed or not.&lt;/span&gt;
&lt;span class="n"&gt;EXIT_CODE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;online&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;New instance started. DNS: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;PrivateDns&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Status&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;setup_failed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Error creating instance. See the Opsworks console for more information.&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;EXIT_CODE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# Get the log file URL to show&lt;/span&gt;
&lt;span class="n"&gt;log_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe_commands&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;instance_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inst&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;InstanceId&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Commands&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;LogUrl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Log file for this instance is &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;log_file&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Reverting the revision back to &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;APP_SOURCE_MASTER&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Revision&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;opsworks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update_app&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;app_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;APP_ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;app_source&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;APP_SOURCE_MASTER&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;All Done&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# Exit with either success (0) or failure (1).&lt;/span&gt;
&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EXIT_CODE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once this script is in place, you should also check the 'This build is parametrized' checkbox and select a &lt;code&gt;String Parameter&lt;/code&gt; with the name &lt;code&gt;Branch&lt;/code&gt; and default value with &lt;code&gt;master&lt;/code&gt;. Save the job - you should now be able to click the &lt;code&gt;Build with parameters&lt;/code&gt; option presented by Jenkins, optionally change the branch and then run the job. If things go well, Jenkins should give you the &lt;em&gt;PrivateDns&lt;/em&gt; URL for the instance.&lt;/p&gt;</content><category term="aws"></category><category term="python"></category></entry><entry><title>A quick Dockerfile to install Oracle's Java7 and Maven.</title><link href="/dockerfile-oracle-java-maven.html" rel="alternate"></link><published>2014-04-04T13:30:00-07:00</published><updated>2014-04-04T13:30:00-07:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2014-04-04:/dockerfile-oracle-java-maven.html</id><summary type="html">&lt;p&gt;A quick Dockerfile to install Oracle's Java7 and Maven.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here's a quick Dockerfile to install Oracle's Java 7 and Maven.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FROM dockerfile/java

&lt;span class="c1"&gt;# Install maven&lt;/span&gt;
RUN apt-get update
RUN apt-get install -y maven
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This uses the the &lt;a href="https://index.docker.io/u/dockerfile/java/"&gt;trusted java build&lt;/a&gt; base image to install Java and then just installs the default maven version delivered with Ubuntu 12.04 (which seems to be &lt;strong&gt;3.0.4-2&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Anyways, real simple Dockerfile - but it helps out when building Java code.&lt;/p&gt;</content><category term="docker"></category><category term="java"></category></entry><entry><title>Setting up a Pelican blog using Dropbox and Github Pages</title><link href="/pelica-dropbox-github-blog.html" rel="alternate"></link><published>2014-03-30T08:30:00-07:00</published><updated>2014-03-30T08:30:00-07:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2014-03-30:/pelica-dropbox-github-blog.html</id><summary type="html">&lt;p&gt;How I went about setting up my Pelican powered blog with the help of Dropbox and Github pages.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just wanted to walk through how I set this blog up - both as a &lt;em&gt;How to&lt;/em&gt; for others and for posterity. If it isn't obvious yet, this blog is powered by &lt;a href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; (the version I'm using as of this writing in 3.3) and is hosted on &lt;a href="http://pages.github.com/"&gt;Github pages&lt;/a&gt;. What isn't obvious yet is that, in the background, &lt;a href="http://dropbox.com"&gt;Dropbox&lt;/a&gt; is helping out too.&lt;/p&gt;
&lt;p&gt;So the structure I ended up following was to create a new folder in my Dropbox folder - I called it &lt;code&gt;gh-blog&lt;/code&gt;. Once in this folder, I follow the following steps:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo pip install virtualenv virtualenvwrapper 
// setup virtualenvwrapper
mkvirtualenv --no-site-packages gh-blog
pip install pelican Fabric
&lt;span class="nb"&gt;cd&lt;/span&gt; ~/Dropbox/gh-blog
pelican-quickstart // I mostly followed the defaults
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After this, I had a 'base' pelican blog setup. I tried running &lt;code&gt;fab build&lt;/code&gt; followed by &lt;code&gt;fab serve&lt;/code&gt; and going to &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; but as expected, no blog posts were shown.&lt;/p&gt;
&lt;p&gt;At this stage, I logged into github and created my public repository matching my username (rmanocha.github.com). Once this was done, I cloned this repository inside the &lt;code&gt;gh-blog&lt;/code&gt; folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~/Dropbox/gh-blog
git clone git@github.com:rmanocha/rmanocha.github.com.git
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, I updated the fabfile (&lt;code&gt;fabfile.py&lt;/code&gt;) to make the newly cloned folder the output folder (this is required since the output folder is where all the HTML and CSS generated by pelican is stored; and this is what is required by github to render your blog). My final fabfile.py looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;fabric.api&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;fabric.contrib.project&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;project&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="c1"&gt;# Local path configuration (can be absolute or relative to fabfile)&lt;/span&gt;
&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deploy_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rmanocha.github.com&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;DEPLOY_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deploy_path&lt;/span&gt;
&lt;span class="n"&gt;GITHUB_REPO&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;git@github.com:rmanocha/rmanocha.github.com.git&amp;#39;&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;clean&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DEPLOY_PATH&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rm -rf {deploy_path}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;git clone &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;GITHUB_REPO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pelican -s pelicanconf.py -o {deploy_path} content&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rebuild&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;clean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;regenerate&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pelican -r -s pelicanconf.py&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;serve&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cd {deploy_path} &amp;amp;&amp;amp; python -m SimpleHTTPServer&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;env&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;reserve&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;serve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;preview&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pelican -s publishconf.py&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You will need to change the &lt;code&gt;env.deploy_path&lt;/code&gt; and &lt;code&gt;GITHUB_REPO&lt;/code&gt; variables above. I haven't yet cleaned up the &lt;code&gt;preview&lt;/code&gt; and &lt;code&gt;regenerate&lt;/code&gt; commands - I haven't really used them yet. As you will notice in the &lt;code&gt;build&lt;/code&gt; command above, the 'content' we're going to generate is going to be stored in the &lt;code&gt;content&lt;/code&gt; folder. So essentially, this is where all your blog posts will go.&lt;/p&gt;
&lt;p&gt;After adding a new post within the &lt;code&gt;content&lt;/code&gt; folder, I ran &lt;code&gt;fab rebuild&lt;/code&gt; followed by a &lt;code&gt;fab serve&lt;/code&gt;. Browsing to &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt; should now show you your new blog post. After making sure everything looked good, I just did the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~/Dropbox/gh-blog/rmanocha.github.com
git add .
git commit -av -m &lt;span class="s2"&gt;&amp;quot;first blog post&amp;quot;&lt;/span&gt;
git push
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Tada. I was now able to browse to &lt;a href="http://rmanocha.github.io"&gt;http://rmanocha.github.io&lt;/a&gt; and see my freshly minted blog post.&lt;/p&gt;
&lt;p&gt;This setup allowed me to store my raw, markdown formatted blog posts somewhere (I didn't want to pay for a github repository) that was backed up and (sort of) version controlled while still publishing the blog posts I wanted to, to github. So far, things seem to be working smoothly.&lt;/p&gt;</content><category term="pelican"></category><category term="github"></category></entry><entry><title>Hello World</title><link href="/hello-world.html" rel="alternate"></link><published>2014-03-29T10:40:00-07:00</published><updated>2014-03-29T10:40:00-07:00</updated><author><name>Rishabh Manocha</name></author><id>tag:None,2014-03-29:/hello-world.html</id><summary type="html">&lt;p&gt;New blog (yet again)&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hello World!! Here I am again, with yet another blog. My previous blogs (here's &lt;a href="http://mylifelogontheweb.blogspot.com/"&gt;one&lt;/a&gt; - the other was on posterous :( ). I'm hoping this one sticks around though.&lt;/p&gt;
&lt;p&gt;Among other things, I plan to use this space to write mostly about various things I learn while writing software in different languages. Given than I'm currently playing around with &lt;a href="http://golang.org"&gt;Go&lt;/a&gt;, you might be reading a lot about that :).&lt;/p&gt;
&lt;p&gt;Here's hoping I make something of this blog.&lt;/p&gt;</content></entry></feed>